{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be660e5",
   "metadata": {},
   "source": [
    "Scan the dataset and create mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35f7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scan_dataset(root_dir):\n",
    "    \"\"\"\n",
    "    root_dir: 'data/train' or 'data/val' or 'data/test'\n",
    "    Returns: list of image paths and their class names\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in sorted(os.listdir(root_dir)):\n",
    "        class_folder = os.path.join(root_dir, class_name)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "        for fname in sorted(os.listdir(class_folder)):\n",
    "            if fname.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                image_paths.append(os.path.join(class_folder, fname))\n",
    "                labels.append(class_name)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "train_paths, train_labels = scan_dataset(\"data/train\")\n",
    "val_paths, val_labels = scan_dataset(\"data/val\")\n",
    "test_paths, _ = scan_dataset(\"data/test\")  \n",
    "\n",
    "# Build a mapping\n",
    "classes = sorted(set(train_labels))  # ['eiffel','stonehenge',...]\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "# Map labels to integers\n",
    "train_labels_idx = [class_to_idx[c] for c in train_labels]\n",
    "val_labels_idx = [class_to_idx[c] for c in val_labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff458bda",
   "metadata": {},
   "source": [
    "Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a2d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels=None, transform=None): # Stores image paths, labels, and transforms.\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels  # None for test set\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self): # returns the number of images in the dataset.\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx): # loads and returns an image and its label (if available).\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return img, label\n",
    "        else:\n",
    "            return img, img_path  # return path for query images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ef9f5",
   "metadata": {},
   "source": [
    "Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d863621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d12e9",
   "metadata": {},
   "source": [
    "DataLoaders\n",
    "\n",
    "train_loader → used for global/local feature training. <br>\n",
    "val_loader → used for monitoring accuracy / mAP. <br>\n",
    "test_loader → used for retrieval / feature extraction, returns image path so you can save .npy or .npz. <br>\n",
    "You can easily modify image size to match your DELF / DELG backbone input (e.g., 512, 1024). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfba9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = LandmarkDataset(train_paths, train_labels_idx, transform=train_transform)\n",
    "val_dataset = LandmarkDataset(val_paths, val_labels_idx, transform=val_transform)\n",
    "test_dataset = LandmarkDataset(test_paths, labels=None, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be189702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first item\n",
    "img, label = train_dataset[0]\n",
    "print(\"Image object:\", img)\n",
    "print(\"Label ID:\", label)\n",
    "\n",
    "# Print shape if it is a tensor\n",
    "print(\"Image shape:\", img.shape)  # [C,H,W]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
