{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be660e5",
   "metadata": {},
   "source": [
    "Scan the dataset and create mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35f7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scan_dataset(root_dir):\n",
    "    \"\"\"\n",
    "    root_dir: 'data/train' or 'data/val' or 'data/test'\n",
    "    Returns: list of image paths and their class names\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in sorted(os.listdir(root_dir)):\n",
    "        class_folder = os.path.join(root_dir, class_name)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "        for fname in sorted(os.listdir(class_folder)):\n",
    "            if fname.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                image_paths.append(os.path.join(class_folder, fname))\n",
    "                labels.append(class_name)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "train_paths, train_labels = scan_dataset(\"data/train\")\n",
    "val_paths, val_labels = scan_dataset(\"data/val\")\n",
    "test_paths, _ = scan_dataset(\"data/test\")  \n",
    "\n",
    "# Build a mapping\n",
    "classes = sorted(set(train_labels))  # ['eiffel','stonehenge',...]\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "# Map labels to integers\n",
    "train_labels_idx = [class_to_idx[c] for c in train_labels]\n",
    "val_labels_idx = [class_to_idx[c] for c in val_labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff458bda",
   "metadata": {},
   "source": [
    "Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a2d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels=None, transform=None): # Stores image paths, labels, and transforms.\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels  # None for test set\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self): # returns the number of images in the dataset.\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx): # loads and returns an image and its label (if available).\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return img, label\n",
    "        else:\n",
    "            return img, img_path  # return path for query images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ef9f5",
   "metadata": {},
   "source": [
    "Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d863621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d12e9",
   "metadata": {},
   "source": [
    "DataLoaders\n",
    "\n",
    "train_loader → used for global/local feature training. <br>\n",
    "val_loader → used for monitoring accuracy / mAP. <br>\n",
    "test_loader → used for retrieval / feature extraction, returns image path so you can save .npy or .npz. <br>\n",
    "You can easily modify image size to match your DELF / DELG backbone input (e.g., 512, 1024). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfba9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = LandmarkDataset(train_paths, train_labels_idx, transform=train_transform)\n",
    "val_dataset = LandmarkDataset(val_paths, val_labels_idx, transform=val_transform)\n",
    "test_dataset = LandmarkDataset(test_paths, labels=None, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be189702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first item\n",
    "img, label = train_dataset[0]\n",
    "print(\"Image object:\", img)\n",
    "print(\"Label ID:\", label)\n",
    "\n",
    "# Print shape if it is a tensor\n",
    "print(\"Image shape:\", img.shape)  # [C,H,W]\n"
   ]
<<<<<<< Updated upstream
=======
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7d7e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3198, -0.3198, -0.3198,  ..., -0.7479, -0.7479, -0.7308],\n",
       "          [-0.3027, -0.3027, -0.3027,  ..., -0.7308, -0.7308, -0.6794],\n",
       "          [-0.2684, -0.2684, -0.2684,  ..., -0.6623, -0.6623, -0.6452],\n",
       "          ...,\n",
       "          [ 0.9646,  0.7419,  0.6906,  ..., -0.0287, -0.0458, -0.3541],\n",
       "          [ 0.8104,  0.8104,  0.8618,  ..., -0.0801, -0.1486, -0.5082],\n",
       "          [ 0.5022,  0.5364,  0.5707,  ...,  0.0741, -0.0287, -0.3198]],\n",
       " \n",
       "         [[ 0.6254,  0.6254,  0.6254,  ...,  0.3277,  0.3277,  0.3452],\n",
       "          [ 0.6779,  0.6779,  0.6779,  ...,  0.3452,  0.3452,  0.3803],\n",
       "          [ 0.7129,  0.7129,  0.7129,  ...,  0.3452,  0.3452,  0.3803],\n",
       "          ...,\n",
       "          [ 1.6583,  1.4657,  1.4657,  ...,  1.0455,  1.0105,  0.7304],\n",
       "          [ 1.5357,  1.5532,  1.6583,  ...,  0.9580,  0.8880,  0.5903],\n",
       "          [ 1.3957,  1.4657,  1.4832,  ...,  1.1155,  1.0805,  0.7829]],\n",
       " \n",
       "         [[ 1.5071,  1.5071,  1.5071,  ...,  1.3677,  1.3677,  1.4025],\n",
       "          [ 1.5594,  1.5594,  1.5594,  ...,  1.4025,  1.4025,  1.4200],\n",
       "          [ 1.5942,  1.5942,  1.5942,  ...,  1.4200,  1.4200,  1.4374],\n",
       "          ...,\n",
       "          [ 2.2914,  2.0823,  2.0823,  ...,  1.9254,  1.9428,  1.6814],\n",
       "          [ 2.2391,  2.2566,  2.3088,  ...,  1.8557,  1.8557,  1.5594],\n",
       "          [ 2.0823,  2.1520,  2.2043,  ...,  2.0474,  2.0300,  1.7685]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4456cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images copied: 2872\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def combine_and_rename_images(source_root='data', target_root='dataset'):\n",
    "    \"\"\"\n",
    "    Collect all images from source_root (train/val/test) and save into target_root\n",
    "    with unique names to avoid overwriting.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_root):\n",
    "        os.makedirs(target_root)\n",
    "    \n",
    "    count = 0\n",
    "    for split in ['train']:\n",
    "        split_path = os.path.join(source_root, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            continue\n",
    "        \n",
    "        for class_folder in os.listdir(split_path):\n",
    "            class_path = os.path.join(split_path, class_folder)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "                \n",
    "                # Create new unique name\n",
    "                ext = os.path.splitext(img_name)[1]  # keep original extension\n",
    "                new_name = f\"{split}_{class_folder}_{count}{ext}\"\n",
    "                new_path = os.path.join(target_root, new_name)\n",
    "                \n",
    "                shutil.copy(img_path, new_path)\n",
    "                count += 1\n",
    "    \n",
    "    print(f\"Total images copied: {count}\")\n",
    "\n",
    "# Example usage:\n",
    "combine_and_rename_images(source_root='data', target_root='dataset')\n"
   ]
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
