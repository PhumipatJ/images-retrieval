{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcad1944",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af7b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scan_dataset(root_dir):\n",
    "    \"\"\"\n",
    "    root_dir: 'data/train' or 'data/val' or 'data/test'\n",
    "    Returns: list of image paths and their class names\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in sorted(os.listdir(root_dir)):\n",
    "        class_folder = os.path.join(root_dir, class_name)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "        for fname in sorted(os.listdir(class_folder)):\n",
    "            if fname.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                image_paths.append(os.path.join(class_folder, fname))\n",
    "                labels.append(class_name)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "train_paths, train_labels = scan_dataset(\"data/train\")\n",
    "val_paths, val_labels = scan_dataset(\"data/val\")\n",
    "test_paths, _ = scan_dataset(\"data/test\")  \n",
    "\n",
    "# Build a mapping\n",
    "classes = sorted(set(train_labels))  # ['eiffel','stonehenge',...]\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "# Map labels to integers\n",
    "train_labels_idx = [class_to_idx[c] for c in train_labels]\n",
    "val_labels_idx = [class_to_idx[c] for c in val_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bfa0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels=None, transform=None): # Stores image paths, labels, and transforms.\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels  # None for test set\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self): # returns the number of images in the dataset.\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx): # loads and returns an image and its label (if available).\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return img, label\n",
    "        else:\n",
    "            return img, img_path  # return path for query images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a426ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e72c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = LandmarkDataset(train_paths, train_labels_idx, transform=train_transform)\n",
    "val_dataset = LandmarkDataset(val_paths, val_labels_idx, transform=val_transform)\n",
    "test_dataset = LandmarkDataset(test_paths, labels=None, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedca399",
   "metadata": {},
   "source": [
    "DELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be7d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866023d",
   "metadata": {},
   "source": [
    "Define DELF Backbone (ResNet50) <br>\n",
    "DELF uses a CNN backbone (ResNet50 in TensorFlow). We can remove the fully connected layers and use feature maps from the last convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELFBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(DELFBackbone, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Take only convolutional layers (exclude avgpool and fc)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B,3,H,W]\n",
    "        feature_map = self.features(x)  # [B, 2048, H/32, W/32]\n",
    "        return feature_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4a5ba",
   "metadata": {},
   "source": [
    "GeM Pooling Layer (for global features if needed) <br>\n",
    "DELF paper uses GeM pooling for global descriptors. For local features, we may skip or just use attention over feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3.0, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x.clamp(min=self.eps).pow(self.p), (x.size(-2), x.size(-1))).pow(1./self.p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d0891",
   "metadata": {},
   "source": [
    "Simple Attention Module (optional) <br>\n",
    "DELF applies attention to feature maps to pick keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels=2048):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 512, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(512, 1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        att = F.relu(self.conv1(x))\n",
    "        att = torch.sigmoid(self.conv2(att))  # attention map: [B,1,H,W]\n",
    "        return att\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068d130",
   "metadata": {},
   "source": [
    "DELF Model Combining Backbone + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELF(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(DELF, self).__init__()\n",
    "        self.backbone = DELFBackbone(pretrained)\n",
    "        self.attention = AttentionModule()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B,3,H,W]\n",
    "        fmap = self.backbone(x)               # [B,2048,H/32,W/32]\n",
    "        att_map = self.attention(fmap)        # [B,1,H/32,W/32]\n",
    "        descriptors = fmap * att_map           # apply attention\n",
    "        return descriptors, att_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcafac3",
   "metadata": {},
   "source": [
    "Feature Extraction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DELF(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example DataLoader\n",
    "# from dataset import train_loader\n",
    "features_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        desc, att = model(images)\n",
    "        # flatten spatial locations\n",
    "        B,C,H,W = desc.shape\n",
    "        desc = desc.view(B,C,H*W).permute(0,2,1)  # [B, H*W, C]\n",
    "        features_list.append(desc.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f950111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, device, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels_or_paths in tqdm(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs)\n",
    "            \n",
    "            # Save features per image\n",
    "            for i in range(imgs.size(0)):\n",
    "                img_name = labels_or_paths[i] if isinstance(labels_or_paths[i], str) else str(labels_or_paths[i])\n",
    "                \n",
    "                if 'global' in feats:\n",
    "                    gfeat = feats['global'][i].cpu().numpy()\n",
    "                    np.save(os.path.join(output_dir, f\"{img_name}_global.npy\"), gfeat)\n",
    "                \n",
    "                if 'local' in feats:\n",
    "                    ldesc = feats['local']['descriptors'][i].cpu().numpy()  # [num_kp, C]\n",
    "                    att = feats['local']['attention'][i].cpu().numpy()\n",
    "                    np.savez(os.path.join(output_dir, f\"{img_name}_local.npz\"),\n",
    "                             descriptors=ldesc, attention=att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DELG(pretrained=True, use_global=True, use_local=True).to(device)\n",
    "\n",
    "# Extract features for train, val, test\n",
    "extract_features(model, train_loader, device, output_dir=\"./features/train\")\n",
    "extract_features(model, val_loader, device, output_dir=\"./features/val\")\n",
    "extract_features(model, test_loader, device, output_dir=\"./features/test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
