{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f73f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from DELG_Class import DELG ,DELGBackbone,GeM, AttentionModule\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ded28",
   "metadata": {},
   "source": [
    "# Load the DELG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ad474",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your fine-tuned DELG model\n",
    "model = DELG(pretrained=False, use_global=True, use_local=True).to(device)\n",
    "classifier = torch.nn.Linear(2048, 18).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"delg_global_finetune_local_pretrained.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "model.eval()\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d05da",
   "metadata": {},
   "source": [
    "# Prepare image dataset (all gallery/query images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703db6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scan_dataset(root_dir):\n",
    "    \"\"\"\n",
    "    root_dir: 'data/train' or 'data/val' or 'data/test'\n",
    "    Returns: list of image paths and their class names\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in sorted(os.listdir(root_dir)):\n",
    "        class_folder = os.path.join(root_dir, class_name)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "        for fname in sorted(os.listdir(class_folder)):\n",
    "            if fname.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                image_paths.append(os.path.join(class_folder, fname))\n",
    "                labels.append(class_name)\n",
    "    \n",
    "    return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35038b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels=None, transform=None): # Stores image paths, labels, and transforms.\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels  # None for test set\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self): # returns the number of images in the dataset.\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx): # loads and returns an image and its label (if available).\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return img, label\n",
    "        else:\n",
    "            return img, img_path  # return path for query images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Replace with your dataset paths\n",
    "all_image_paths, _ = scan_dataset(\"data/all_images\")  # all images for retrieval\n",
    "dataset = LandmarkDataset(all_image_paths, labels=None, transform=val_transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315330d",
   "metadata": {},
   "source": [
    "# Extract Global Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_features = []\n",
    "image_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, paths in tqdm(loader, desc=\"Extracting global descriptors\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model(imgs)['global']           # [B, 2048]\n",
    "        feats = F.normalize(feats, p=2, dim=1) # L2 normalize\n",
    "        global_features.append(feats.cpu())\n",
    "        image_paths.extend(paths)\n",
    "\n",
    "global_features = torch.cat(global_features, dim=0)  # [N_images, 2048]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbc2fb",
   "metadata": {},
   "source": [
    "# Top-k candidate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c623b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_candidates(query_feat, all_feats, all_paths, k=10):\n",
    "    # query_feat: [2048] tensor\n",
    "    sims = F.cosine_similarity(query_feat.unsqueeze(0), all_feats)  # [N_images]\n",
    "    topk_idx = torch.topk(sims, k=k).indices.cpu().numpy()\n",
    "    return [all_paths[i] for i in topk_idx], sims[topk_idx]\n",
    "\n",
    "# Example: pick the first image as query\n",
    "query_feat = global_features[0]\n",
    "query_path = image_paths[0]\n",
    "topk_paths, topk_scores = get_topk_candidates(query_feat, global_features, image_paths, k=10)\n",
    "print(\"Query image:\", query_path)\n",
    "print(\"Top-k candidates:\", topk_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2102d",
   "metadata": {},
   "source": [
    "# Extract local descriptors for top-k + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f2d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_desc(image_paths):\n",
    "    local_descs = []\n",
    "    attention_maps = []\n",
    "    dataset_tmp = LandmarkDataset(image_paths, labels=None, transform=val_transform)\n",
    "    loader_tmp = DataLoader(dataset_tmp, batch_size=16, shuffle=False, num_workers=2)\n",
    "    with torch.no_grad():\n",
    "        for imgs, paths in loader_tmp:\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)['local']\n",
    "            desc = out['descriptors'].cpu()  # [B, H*W, 2048]\n",
    "            att = out['attention'].cpu()     # [B, 1, H, W]\n",
    "            local_descs.append(desc)\n",
    "            attention_maps.append(att)\n",
    "    local_descs = torch.cat(local_descs, dim=0)\n",
    "    attention_maps = torch.cat(attention_maps, dim=0)\n",
    "    return local_descs, attention_maps\n",
    "\n",
    "# Query local descriptors\n",
    "query_local, query_att = extract_local_desc([query_path])\n",
    "# Top-k local descriptors\n",
    "topk_local, topk_att = extract_local_desc(topk_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e84d20",
   "metadata": {},
   "source": [
    "# Geometric verification / re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f203af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified re-ranking: compute cosine similarity between local descriptors\n",
    "def rerank(query_desc, topk_descs, topk_paths, top_n=5):\n",
    "    scores = []\n",
    "    query_desc_flat = query_desc.view(-1, query_desc.size(-1))  # [H*W, 2048]\n",
    "    for i, desc in enumerate(topk_descs):\n",
    "        desc_flat = desc.view(-1, desc.size(-1))\n",
    "        sim_matrix = F.cosine_similarity(query_desc_flat.unsqueeze(1), desc_flat.unsqueeze(0), dim=2)\n",
    "        score = sim_matrix.max(dim=1)[0].mean().item()  # max per query point\n",
    "        scores.append((score, topk_paths[i]))\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [p for s, p in scores[:top_n]]\n",
    "\n",
    "topn_paths = rerank(query_local[0], topk_local, topk_paths, top_n=5)\n",
    "print(\"Top-n after re-ranking:\", topn_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
